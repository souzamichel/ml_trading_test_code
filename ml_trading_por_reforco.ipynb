{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPecYsSroqKtqxS2h8yrFhY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/souzamichel/ml_trading_test_code/blob/evolu%C3%A7%C3%A3o/ml_trading_por_reforco.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "collapsed": true,
        "id": "oxVXyH1exOP-",
        "outputId": "5d14d434-b96a-446f-f468-0dbed9115e48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Env] Inicializando TradingEnv: 1509 candles, ATR window=14, SL_mult=0.5, TP_mult=4.0\n",
            "[Agent] state_dim=43, actions=3\n",
            "[Train] 1000 episódios...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-3827302270.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-8-3827302270.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    415\u001b[0m                          n_actions=3)\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         env_te = TradingEnv(te_p, te_h, te_l,\n",
            "\u001b[0;32m/tmp/ipython-input-8-3827302270.py\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(env, agent, n_episodes)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-8-3827302270.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mq_targets\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnext_q_values\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1733\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrapped_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "class TradingEnv:\n",
        "    \"\"\"\n",
        "    Ambiente de trading com três ações: 0=Manter, 1=Comprar, 2=Vender.\n",
        "    Estado enriquecido = (bin retorno instantâneo, posição atual, bin média móvel de retorno).\n",
        "    Usa stops dinâmicos baseados em ATR, custos fixos, custos percentuais, slippage e recompensa percentual.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        prices,\n",
        "        highs,\n",
        "        lows,\n",
        "        n_return_bins=10,\n",
        "        initial_capital=1_000,\n",
        "        lot_size=1,\n",
        "        transaction_cost_pct=0.001,\n",
        "        slippage_pct=0.001,\n",
        "        atr_window=14,\n",
        "        sl_atr_mult=0.5,\n",
        "        tp_atr_mult=4.0,\n",
        "        risk_aversion=0.0,\n",
        "        fixed_trade_cost=0.5,           # ajuste: custo fixo por trade\n",
        "        momentum_filter=True,           # ajuste: sinal de confirmação\n",
        "        ma_window=5\n",
        "    ):\n",
        "        print(f\"[Env] Inicializando TradingEnv: {len(prices)} candles, ATR window={atr_window}, SL_mult={sl_atr_mult}, TP_mult={tp_atr_mult}\")\n",
        "        self.prices = np.asarray(prices).flatten()\n",
        "        self.highs  = np.asarray(highs).flatten()\n",
        "        self.lows   = np.asarray(lows).flatten()\n",
        "        if self.prices.size < atr_window + 1:\n",
        "            raise ValueError(\"Série muito curta para ATR.\")\n",
        "\n",
        "        # Retornos e bins\n",
        "        diffs       = np.diff(self.prices)\n",
        "        prev_prices = self.prices[:-1]\n",
        "        self.returns = diffs / prev_prices\n",
        "        self.n_bins  = n_return_bins\n",
        "        self.bins    = np.linspace(self.returns.min(),\n",
        "                                   self.returns.max(),\n",
        "                                   self.n_bins + 1)\n",
        "\n",
        "        # Média móvel de retornos\n",
        "        self.ma_window = ma_window\n",
        "        self.return_ma = pd.Series(self.returns)\\\n",
        "                            .rolling(window=ma_window, min_periods=1)\\\n",
        "                            .mean()\\\n",
        "                            .to_numpy()\n",
        "\n",
        "        # Parâmetros de trade\n",
        "        self.initial_capital      = initial_capital\n",
        "        self.lot_size             = lot_size\n",
        "        self.transaction_cost_pct = transaction_cost_pct\n",
        "        self.slippage_pct         = slippage_pct\n",
        "        self.fixed_trade_cost     = fixed_trade_cost    # ajuste\n",
        "\n",
        "        # Ajustes extras\n",
        "        self.momentum_filter = momentum_filter         # ajuste\n",
        "\n",
        "        # Parâmetros ATR stops\n",
        "        self.atr_window  = atr_window\n",
        "        self.sl_atr_mult = sl_atr_mult\n",
        "        self.tp_atr_mult = tp_atr_mult\n",
        "\n",
        "        # Calcular ATR\n",
        "        tr1    = self.highs[1:] - self.lows[1:]\n",
        "        tr2    = np.abs(self.highs[1:] - self.prices[:-1])\n",
        "        tr3    = np.abs(self.lows[1:]  - self.prices[:-1])\n",
        "        tr_max = np.maximum.reduce([tr1, tr2, tr3])\n",
        "        atr    = pd.Series(tr_max)\\\n",
        "                    .rolling(window=atr_window, min_periods=1)\\\n",
        "                    .mean()\\\n",
        "                    .bfill()\n",
        "        self.atr = np.concatenate([[atr.iloc[0]], atr.to_numpy()])\n",
        "        # ajuste: limiar mínimo de volatilidade para entradas\n",
        "        self.atr_median = np.median(self.atr)\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.t           = 0\n",
        "        self.position    = 0\n",
        "        self.cash        = self.initial_capital\n",
        "        self.inventory   = 0\n",
        "        self.entry_price = None\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        ret     = self.returns[self.t] if self.t < len(self.returns) else self.returns[-1]\n",
        "        bin_idx = np.clip(np.digitize(ret, self.bins) - 1, 0, self.n_bins - 1)\n",
        "\n",
        "        ma_ret     = self.return_ma[self.t] if self.t < len(self.return_ma) else self.return_ma[-1]\n",
        "        ma_bin_idx = np.clip(np.digitize(ma_ret, self.bins) - 1, 0, self.n_bins - 1)\n",
        "\n",
        "        pos_idx = self.position\n",
        "        return (bin_idx, pos_idx, ma_bin_idx)\n",
        "\n",
        "    def step(self, action):\n",
        "        price_prev      = self.prices[self.t]\n",
        "        self.t         += 1\n",
        "        price_now       = self.prices[self.t]\n",
        "        prev_portfolio = self.cash + self.inventory * price_prev\n",
        "\n",
        "        # volume pode ser fixo ou proporcional\n",
        "        volume = self.lot_size\n",
        "\n",
        "        # BUY: só se cumprir filtros de volatilidade e momentum\n",
        "        if action == 1:\n",
        "            current_atr = self.atr[self.t]\n",
        "            # ajuste: só compra se ATR > mediana e retorno anterior positivo\n",
        "            prev_ret = self.returns[self.t-1] if self.t-1 < len(self.returns) else 0\n",
        "            if current_atr > self.atr_median and (not self.momentum_filter or prev_ret > 0):\n",
        "                buy_price  = price_now * (1 + self.slippage_pct)\n",
        "                cost_pct   = buy_price * volume * (1 + self.transaction_cost_pct)\n",
        "                total_cost = cost_pct + self.fixed_trade_cost\n",
        "                if self.cash >= total_cost:\n",
        "                    self.cash       -= total_cost\n",
        "                    self.inventory  += volume\n",
        "                    self.position    = 1\n",
        "                    self.entry_price = price_now\n",
        "\n",
        "        # SELL: padrão, mas com custo fixo\n",
        "        elif action == 2 and self.inventory >= volume:\n",
        "            sell_price = price_now * (1 - self.slippage_pct)\n",
        "            proceeds_pct = sell_price * volume * (1 - self.transaction_cost_pct)\n",
        "            total_proceeds = proceeds_pct - self.fixed_trade_cost\n",
        "            self.cash      += total_proceeds\n",
        "            self.inventory -= volume\n",
        "            self.position   = 0\n",
        "            self.entry_price = None\n",
        "\n",
        "        # Stops dinâmicos (SL / TP)\n",
        "        if self.position == 1 and self.entry_price is not None:\n",
        "            atr      = self.atr[self.t]\n",
        "            sl_price = self.entry_price - self.sl_atr_mult * atr\n",
        "            tp_price = self.entry_price + self.tp_atr_mult * atr\n",
        "            if price_now <= sl_price or price_now >= tp_price:\n",
        "                exit_price     = price_now * (1 - self.slippage_pct)\n",
        "                exit_proceeds  = exit_price * volume * (1 - self.transaction_cost_pct)\n",
        "                exit_proceeds -= self.fixed_trade_cost      # ajuste\n",
        "                self.cash       += exit_proceeds\n",
        "                self.inventory  -= volume\n",
        "                self.position    = 0\n",
        "                self.entry_price = None\n",
        "\n",
        "        curr_portfolio = self.cash + self.inventory * price_now\n",
        "        raw_reward     = curr_portfolio - prev_portfolio\n",
        "        reward         = raw_reward / prev_portfolio if prev_portfolio > 0 else 0.0\n",
        "\n",
        "        done       = (self.t == len(self.prices) - 1)\n",
        "        next_state = self._get_state()\n",
        "        return next_state, reward, done\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    Agente DQN com epsilon-greedy e replay buffer.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_return_bins,\n",
        "        n_position_states,\n",
        "        n_actions,\n",
        "        lr=1e-3,\n",
        "        gamma=0.99,\n",
        "        epsilon=1.0,\n",
        "        epsilon_decay=0.9999,\n",
        "        epsilon_min=0.01,\n",
        "        batch_size=64,\n",
        "        target_update=200,\n",
        "        memory_size=10000,\n",
        "        device=None\n",
        "    ):\n",
        "        state_dim = 2 * n_return_bins + n_position_states\n",
        "        print(f\"[Agent] state_dim={state_dim}, actions={n_actions}\")\n",
        "        self.n_return_bins     = n_return_bins\n",
        "        self.n_position_states = n_position_states\n",
        "        self.state_dim         = state_dim\n",
        "        self.n_actions         = n_actions\n",
        "        self.gamma             = gamma\n",
        "        self.epsilon           = epsilon\n",
        "        self.epsilon_decay     = epsilon_decay\n",
        "        self.epsilon_min       = epsilon_min\n",
        "        self.batch_size        = batch_size\n",
        "        self.target_update     = target_update\n",
        "        self.memory            = deque(maxlen=memory_size)\n",
        "        self.steps_done        = 0\n",
        "        self.device            = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        class Net(nn.Module):\n",
        "            def __init__(self, inp, out):\n",
        "                super().__init__()\n",
        "                self.layers = nn.Sequential(\n",
        "                    nn.Linear(inp, 128),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(128, 128),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(128, out)\n",
        "                )\n",
        "            def forward(self, x):\n",
        "                return self.layers(x)\n",
        "\n",
        "        self.policy_net = Net(self.state_dim, n_actions).to(self.device)\n",
        "        self.target_net = Net(self.state_dim, n_actions).to(self.device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.loss_fn   = nn.MSELoss()\n",
        "\n",
        "    def _one_hot(self, state):\n",
        "        vec = np.zeros(self.state_dim, dtype=np.float32)\n",
        "        vec[state[0]] = 1.0\n",
        "        vec[self.n_return_bins + state[1]] = 1.0\n",
        "        offset = self.n_return_bins + self.n_position_states\n",
        "        vec[offset + state[2]] = 1.0\n",
        "        return vec\n",
        "\n",
        "    def select_action(self, state):\n",
        "        self.steps_done += 1\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randrange(self.n_actions)\n",
        "        v = torch.tensor(self._one_hot(state),\n",
        "                         dtype=torch.float32,\n",
        "                         device=self.device).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            q = self.policy_net(v)\n",
        "        return q.argmax().item()\n",
        "\n",
        "    def remember(self, s, a, r, ns, done):\n",
        "        self.memory.append((s, a, r, ns, done))\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states_np      = np.array([self._one_hot(s) for s in states], dtype=np.float32)\n",
        "        next_states_np = np.array([self._one_hot(s) for s in next_states], dtype=np.float32)\n",
        "\n",
        "        states      = torch.from_numpy(states_np).to(self.device)\n",
        "        next_states = torch.from_numpy(next_states_np).to(self.device)\n",
        "        actions     = torch.tensor(actions, device=self.device).unsqueeze(1)\n",
        "        rewards     = torch.tensor(rewards, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
        "        dones       = torch.tensor(dones,   dtype=torch.float32, device=self.device).unsqueeze(1)\n",
        "\n",
        "        q_values      = self.policy_net(states).gather(1, actions)\n",
        "        next_q_values = self.target_net(next_states).max(1)[0].unsqueeze(1)\n",
        "        q_targets     = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "\n",
        "        loss = self.loss_fn(q_values, q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.steps_done % self.target_update == 0:\n",
        "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
        "\n",
        "def train_agent(env, agent, n_episodes=200):\n",
        "    print(f\"[Train] {n_episodes} episódios...\")\n",
        "    history = {'episode_rewards': []}\n",
        "    for ep in range(1, n_episodes + 1):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            agent.learn()\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "        history['episode_rewards'].append(total_reward)\n",
        "        if ep % 50 == 0:\n",
        "            print(f\"[Train] Ep {ep}/{n_episodes}, Rew={total_reward:.2f}, ε={agent.epsilon:.3f}\")\n",
        "    print(\"[Train] Concluído.\")\n",
        "    return history\n",
        "\n",
        "\n",
        "def evaluate_agent(env, agent, exploration=True):\n",
        "    print(\"[Eval] Teste com ε exploratório\" if exploration else \"[Eval] Teste sem exploração\")\n",
        "    orig_eps = agent.epsilon\n",
        "    if exploration:\n",
        "        agent.epsilon = 0.1\n",
        "\n",
        "    state = env.reset()\n",
        "    portfolio        = [env.initial_capital]\n",
        "    action_log       = []\n",
        "    trade_logs       = []\n",
        "    position_hist    = []\n",
        "    entry            = None\n",
        "    price_series     = env.prices.copy()\n",
        "    tc               = env.transaction_cost_pct\n",
        "    sl               = env.slippage_pct\n",
        "    vol              = env.lot_size\n",
        "\n",
        "    cum_reward = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.select_action(state)\n",
        "        action_log.append(action)\n",
        "        next_state, reward, done = env.step(action)\n",
        "\n",
        "        # posição e PnL\n",
        "        position_hist.append(env.position)\n",
        "        if env.position == 1 and entry is None:\n",
        "            entry = {'price': env.entry_price, 'idx': env.t}\n",
        "        if entry and env.position == 0:\n",
        "            idx_exit = env.t\n",
        "            mkt_price = price_series[idx_exit]\n",
        "            buy_p  = entry['price'] * (1 + sl)\n",
        "            cost   = buy_p * vol * (1 + tc)\n",
        "            sell_p = mkt_price      * (1 - sl)\n",
        "            prof   = sell_p * vol   * (1 - tc) - cost\n",
        "            trade_logs.append({\n",
        "                'entry_idx': entry['idx'],\n",
        "                'exit_idx':  idx_exit,\n",
        "                'pnl':       prof\n",
        "            })\n",
        "            entry = None\n",
        "\n",
        "        state = next_state\n",
        "        cum_reward += reward\n",
        "        portfolio.append(env.cash + env.inventory * env.prices[env.t])\n",
        "\n",
        "    if exploration:\n",
        "        agent.epsilon = orig_eps\n",
        "\n",
        "    print(f\"[Eval] Lucro final: {cum_reward:.2f}\")\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot(portfolio, label='Portfólio')\n",
        "    plt.title('Evolução no teste')\n",
        "    plt.show()\n",
        "\n",
        "    df_tr = pd.DataFrame(trade_logs)\n",
        "    print(\"\\nTrades:\\n\", df_tr)\n",
        "    df_tr.to_csv(\"trade_logs.csv\", index=False)\n",
        "\n",
        "    plt.figure(figsize=(12,2))\n",
        "    plt.plot(position_hist, drawstyle='steps-post')\n",
        "    plt.yticks([0,1], ['Flat','Long'])\n",
        "    plt.title('Posição ao longo do tempo')\n",
        "    plt.show()\n",
        "\n",
        "    return portfolio, action_log\n",
        "\n",
        "\n",
        "def compute_metrics(portfolio, action_log):\n",
        "    returns = np.diff(portfolio)\n",
        "    sharpe  = (np.mean(returns) / np.std(returns) * np.sqrt(252)) if np.std(returns)>0 else 0\n",
        "    trades  = sum(1 for a in action_log if a in [1,2])\n",
        "    peak    = np.maximum.accumulate(portfolio)\n",
        "    dd      = peak - portfolio\n",
        "    return sharpe, trades, dd\n",
        "\n",
        "\n",
        "def main():\n",
        "    ticker = \"AAPL\"\n",
        "    start  = \"2015-01-01\"\n",
        "    end    = \"2025-01-01\"\n",
        "    df = yf.download(ticker, start=start, end=end, auto_adjust=False)\n",
        "    prices = df[\"Close\"].to_numpy()\n",
        "    highs  = df[\"High\"].to_numpy()\n",
        "    lows   = df[\"Low\"].to_numpy()\n",
        "\n",
        "    n_bins     = 20\n",
        "    capital    = 1_000\n",
        "    lot        = 1     # lote ajustado a 1 ação\n",
        "    tc, sl     = 0.001, 0.002\n",
        "    atr_w      = 14\n",
        "    sl_mult, tp_mult = 0.5, 4.0  # novos multipliers\n",
        "\n",
        "    N          = len(prices)\n",
        "    train_sz   = int(N*0.6)\n",
        "    test_sz    = int(N*0.2)\n",
        "    step       = test_sz\n",
        "\n",
        "    all_results = []\n",
        "    last_log    = []\n",
        "    last_pf     = []\n",
        "    periods     = 0\n",
        "\n",
        "    for i in range(0, N-train_sz-test_sz+1, step):\n",
        "        periods += 1\n",
        "        tr_p = prices[i : i+train_sz]\n",
        "        tr_h = highs [i : i+train_sz]\n",
        "        tr_l = lows  [i : i+train_sz]\n",
        "        te_p = prices[i+train_sz : i+train_sz+test_sz]\n",
        "        te_h = highs [i+train_sz : i+train_sz+test_sz]\n",
        "        te_l = lows  [i+train_sz : i+train_sz+test_sz]\n",
        "\n",
        "        env_tr = TradingEnv(tr_p, tr_h, tr_l,\n",
        "                            n_return_bins=n_bins,\n",
        "                            initial_capital=capital,\n",
        "                            lot_size=lot,\n",
        "                            transaction_cost_pct=tc,\n",
        "                            slippage_pct=sl,\n",
        "                            atr_window=atr_w,\n",
        "                            sl_atr_mult=sl_mult,\n",
        "                            tp_atr_mult=tp_mult,\n",
        "                            risk_aversion=0.0,\n",
        "                            ma_window=5)\n",
        "\n",
        "        agent = DQNAgent(n_return_bins=n_bins,\n",
        "                         n_position_states=3,\n",
        "                         n_actions=3)\n",
        "\n",
        "        history = train_agent(env_tr, agent, n_episodes=1000)\n",
        "\n",
        "        env_te = TradingEnv(te_p, te_h, te_l,\n",
        "                            n_return_bins=n_bins,\n",
        "                            initial_capital=capital,\n",
        "                            lot_size=lot,\n",
        "                            transaction_cost_pct=tc,\n",
        "                            slippage_pct=sl,\n",
        "                            atr_window=atr_w,\n",
        "                            sl_atr_mult=sl_mult,\n",
        "                            tp_atr_mult=tp_mult,\n",
        "                            risk_aversion=0.0,\n",
        "                            ma_window=5)\n",
        "\n",
        "        pf, log = evaluate_agent(env_te, agent, exploration=True)\n",
        "        all_results.append(pf[-1])\n",
        "        last_log    = log\n",
        "        last_pf     = pf\n",
        "\n",
        "        sharpe, trades, dd = compute_metrics(pf, log)\n",
        "        print(f\"[Metrics] Sharpe={sharpe:.2f}, Trades={trades}\")\n",
        "\n",
        "    print(\"\\nWalk-forward finalizado. Lucros:\", all_results)\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.hist(last_log, bins=[-0.5,0.5,1.5,2.5], rwidth=0.8, edgecolor='k')\n",
        "    plt.xticks([0,1,2], ['Hold','Buy','Sell'])\n",
        "    plt.title('Ações no último teste')\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot(history['episode_rewards'], label='Recompensa por episódio')\n",
        "    plt.plot(last_pf, label='Portfólio (último período)')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}